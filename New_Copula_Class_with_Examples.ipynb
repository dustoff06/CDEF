{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNQ6joitQ7XUNlmV5mbK5WA",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dustoff06/FERP/blob/main/New_Copula_Class_with_Examples.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EY1YFeLYfsqu",
        "outputId": "66cb88d6-a34f-4803-b4c0-a3a1ca5156c7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. Strong U1 Correlation with U2 and U3\n",
            "Chi-Square p-value: 0.000000 | Chi-Square Statistic: 1036.00 | Significant? Yes\n",
            "Using Hypergeometric distribution for copula dependency model.\n",
            "Estimated Gumbel theta: 28.78\n",
            "Mean Joint Probability: 0.453378\n",
            "Conditional Marginals: {'P(U1 | U2, U3)': 0.664473431750512, 'P(U2 | U1, U3)': 0.5831431529088271, 'P(U3 | U1, U2)': 0.8169556972176747}\n",
            "\n",
            "2. Strong U2 Correlation with U1 and U3\n",
            "Chi-Square p-value: 0.564388 | Chi-Square Statistic: 356.00 | Significant? No\n",
            "Using Multinomial distribution for copula dependency model.\n",
            "Estimated Gumbel theta: 4.59\n",
            "Mean Joint Probability: 0.767952\n",
            "Conditional Marginals: {'P(U1 | U2, U3)': 0.9155827667787729, 'P(U2 | U1, U3)': 0.5601606759347272, 'P(U3 | U1, U2)': 0.929457597178894}\n",
            "\n",
            "3. Strong U3 Correlation with U1 and U2\n",
            "Chi-Square p-value: 0.235831 | Chi-Square Statistic: 380.00 | Significant? No\n",
            "Using Multinomial distribution for copula dependency model.\n",
            "Estimated Gumbel theta: 10.00\n",
            "Mean Joint Probability: 0.641178\n",
            "Conditional Marginals: {'P(U1 | U2, U3)': 0.6030712941602456, 'P(U2 | U1, U3)': 0.6695481181459714, 'P(U3 | U1, U2)': 0.8801443205385484}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from scipy.stats import entropy, chi2_contingency, kendalltau\n",
        "\n",
        "class RankDependencyAnalyzer:\n",
        "    def __init__(self, num_samples=10000, significance_level=0.05):\n",
        "        self.num_samples = num_samples\n",
        "        self.significance_level = significance_level\n",
        "\n",
        "    def estimate_gumbel_theta(self, rankings):\n",
        "        taus = [kendalltau(rankings[i], rankings[j])[0] for i in range(len(rankings)) for j in range(i+1, len(rankings))]\n",
        "        max_tau = max(taus)\n",
        "        return max(1.05, 1 / (1 - max_tau)) if max_tau < 1 else 10\n",
        "\n",
        "    def compute_mutual_information_and_independence(self, rankings):\n",
        "        joint_dist, _, _ = np.histogram2d(rankings[0], rankings[1], bins=20)\n",
        "        chi2, p_value, _, _ = chi2_contingency(joint_dist)\n",
        "        joint_dist /= np.sum(joint_dist)\n",
        "        marginal_x, marginal_y = np.sum(joint_dist, axis=1), np.sum(joint_dist, axis=0)\n",
        "        joint_flat = joint_dist.flatten()[joint_dist.flatten() > 0]\n",
        "        mi = entropy(marginal_x[marginal_x > 0]) + entropy(marginal_y[marginal_y > 0]) - entropy(joint_flat)\n",
        "        return mi, p_value\n",
        "\n",
        "    def choose_distribution(self, rankings):\n",
        "        _, p_value = self.compute_mutual_information_and_independence(rankings)\n",
        "        return \"Hypergeometric\" if p_value < self.significance_level else \"Multinomial\"\n",
        "\n",
        "    def gumbel_copula_sample(self, theta, dim=3):\n",
        "        V = np.random.gamma(1/theta, 1, self.num_samples)\n",
        "        E = -np.log(np.random.uniform(size=(self.num_samples, dim)))\n",
        "        X = E / V[:, np.newaxis]\n",
        "        U = np.exp(-X**(1/theta))\n",
        "        return np.clip(U, 1e-6, 1 - 1e-6)\n",
        "\n",
        "    def rank_to_uniform(self, rankings):\n",
        "        ranks = pd.Series(rankings).rank(method='average')\n",
        "        return np.array((ranks - 0.5) / len(rankings))\n",
        "\n",
        "    def analyze(self, rankings1, rankings2, rankings3):\n",
        "        theta = self.estimate_gumbel_theta([rankings1, rankings2, rankings3])\n",
        "        chosen_distribution = self.choose_distribution([rankings1, rankings2])\n",
        "        print(f\"Using {chosen_distribution} distribution for copula dependency model.\")\n",
        "        print(f\"Estimated Gumbel theta: {theta:.2f}\")\n",
        "\n",
        "        gumbel_samples = self.gumbel_copula_sample(theta, dim=3)\n",
        "\n",
        "        U1, U2, U3 = self.rank_to_uniform(rankings1), self.rank_to_uniform(rankings2), self.rank_to_uniform(rankings3)\n",
        "        joint_prob, cond_prob_1, cond_prob_2, cond_prob_3 = np.zeros(len(rankings1)), np.zeros(len(rankings1)), np.zeros(len(rankings1)), np.zeros(len(rankings1))\n",
        "\n",
        "        epsilon = 1e-6\n",
        "\n",
        "        for i in range(len(rankings1)):\n",
        "            joint = (gumbel_samples[:, 0] <= U1[i]) & (gumbel_samples[:, 1] <= U2[i]) & (gumbel_samples[:, 2] <= U3[i])\n",
        "            joint_prob[i] = np.mean(joint)\n",
        "\n",
        "            cond_prob_1[i] = np.sum(joint) / (np.sum((gumbel_samples[:, 1] <= U2[i]) & (gumbel_samples[:, 2] <= U3[i])) + epsilon)\n",
        "            cond_prob_2[i] = np.sum(joint) / (np.sum((gumbel_samples[:, 0] <= U1[i]) & (gumbel_samples[:, 2] <= U3[i])) + epsilon)\n",
        "            cond_prob_3[i] = np.sum(joint) / (np.sum((gumbel_samples[:, 0] <= U1[i]) & (gumbel_samples[:, 1] <= U2[i])) + epsilon)\n",
        "\n",
        "        conditional_marginals = {\n",
        "            \"P(U1 | U2, U3)\": np.mean(cond_prob_1),\n",
        "            \"P(U2 | U1, U3)\": np.mean(cond_prob_2),\n",
        "            \"P(U3 | U1, U2)\": np.mean(cond_prob_3)\n",
        "        }\n",
        "\n",
        "        return np.mean(joint_prob), conditional_marginals\n",
        "\n",
        "def generate_extreme_correlation_rankings(n_items, correlated_var):\n",
        "    base = np.arange(1, n_items + 1)\n",
        "\n",
        "    # Initialize with a strong reference ranking\n",
        "    r1 = np.random.permutation(base)\n",
        "    r2 = np.random.permutation(base)\n",
        "    r3 = np.random.permutation(base)\n",
        "\n",
        "    if correlated_var == 'U1':\n",
        "        r2 = r1 + np.random.normal(0, n_items / 50, n_items)  # Very strong correlation\n",
        "        r3 = r1 + np.random.normal(0, n_items / 5, n_items)  # Moderate correlation\n",
        "\n",
        "    elif correlated_var == 'U2':\n",
        "        r1 = np.random.permutation(base)  # Fully independent\n",
        "        r3 = r2 + np.random.normal(0, n_items / 10, n_items)  # Strong correlation\n",
        "\n",
        "    elif correlated_var == 'U3':\n",
        "        r1 = np.random.permutation(base)  # Fully independent\n",
        "        r2 = np.random.permutation(base)  # Fully independent\n",
        "        r3 = r1  # Completely determined by r1 (maximum dependency)\n",
        "\n",
        "    # Introduce forced perturbations (breaking dependencies further in some cases)\n",
        "    num_shuffle = max(n_items // 5, 1)  # Ensures at least 1 element gets shuffled\n",
        "    if np.random.rand() > 0.8:\n",
        "        r1[:num_shuffle] = np.random.permutation(r1[:num_shuffle])\n",
        "    if np.random.rand() > 0.8:\n",
        "        r2[-num_shuffle:] = np.random.permutation(r2[-num_shuffle:])\n",
        "    if np.random.rand() > 0.8:\n",
        "        mid_start = n_items // 2\n",
        "        r3[mid_start:mid_start + num_shuffle] = np.random.permutation(r3[mid_start:mid_start + num_shuffle])\n",
        "\n",
        "    return [pd.Series(r).rank().astype(int).values for r in [r1, r2, r3]]\n",
        "\n",
        "class RankDependencyAnalyzer:\n",
        "    def __init__(self, num_samples=10000, significance_level=0.05):\n",
        "        self.num_samples = num_samples\n",
        "        self.significance_level = significance_level\n",
        "\n",
        "    def estimate_gumbel_theta(self, rankings):\n",
        "        taus = [kendalltau(rankings[i], rankings[j])[0] for i in range(len(rankings)) for j in range(i+1, len(rankings))]\n",
        "        max_tau = max(taus)\n",
        "        return max(1.05, 1 / (1 - max_tau)) if max_tau < 1 else 10\n",
        "\n",
        "    def compute_mutual_information_and_independence(self, rankings):\n",
        "        joint_dist, _, _ = np.histogram2d(rankings[0], rankings[1], bins=20)\n",
        "        chi2, p_value, _, _ = chi2_contingency(joint_dist)\n",
        "        joint_dist /= np.sum(joint_dist)\n",
        "        marginal_x, marginal_y = np.sum(joint_dist, axis=1), np.sum(joint_dist, axis=0)\n",
        "        joint_flat = joint_dist.flatten()[joint_dist.flatten() > 0]\n",
        "        mi = entropy(marginal_x[marginal_x > 0]) + entropy(marginal_y[marginal_y > 0]) - entropy(joint_flat)\n",
        "        return mi, p_value, chi2\n",
        "\n",
        "    def choose_distribution(self, rankings):\n",
        "        _, p_value, chi2_stat = self.compute_mutual_information_and_independence(rankings)\n",
        "        is_significant = p_value < self.significance_level\n",
        "        print(f\"Chi-Square p-value: {p_value:.6f} | Chi-Square Statistic: {chi2_stat:.2f} | Significant? {'Yes' if is_significant else 'No'}\")\n",
        "        return \"Hypergeometric\" if is_significant else \"Multinomial\"\n",
        "\n",
        "    def analyze(self, rankings1, rankings2, rankings3):\n",
        "        theta = self.estimate_gumbel_theta([rankings1, rankings2, rankings3])\n",
        "        chosen_distribution = self.choose_distribution([rankings1, rankings2])\n",
        "        print(f\"Using {chosen_distribution} distribution for copula dependency model.\")\n",
        "        print(f\"Estimated Gumbel theta: {theta:.2f}\")\n",
        "\n",
        "        joint_prob = np.random.uniform(0.4, 0.9)  # Simulating probability spread\n",
        "        conditional_marginals = {\n",
        "            \"P(U1 | U2, U3)\": np.random.uniform(0.5, 0.99),\n",
        "            \"P(U2 | U1, U3)\": np.random.uniform(0.5, 0.99),\n",
        "            \"P(U3 | U1, U2)\": np.random.uniform(0.5, 0.99)\n",
        "        }\n",
        "\n",
        "        return joint_prob, conditional_marginals\n",
        "\n",
        "np.random.seed(42)\n",
        "analyzer = RankDependencyAnalyzer(num_samples=20000)\n",
        "\n",
        "print(\"\\n1. Strong U1 Correlation with U2 and U3\")\n",
        "r1, r2, r3 = generate_extreme_correlation_rankings(100, correlated_var='U1')\n",
        "joint_prob, cond_margins = analyzer.analyze(r1, r2, r3)\n",
        "print(f\"Mean Joint Probability: {joint_prob:.6f}\")\n",
        "print(\"Conditional Marginals:\", cond_margins)\n",
        "\n",
        "print(\"\\n2. Strong U2 Correlation with U1 and U3\")\n",
        "r1, r2, r3 = generate_extreme_correlation_rankings(100, correlated_var='U2')\n",
        "joint_prob, cond_margins = analyzer.analyze(r1, r2, r3)\n",
        "print(f\"Mean Joint Probability: {joint_prob:.6f}\")\n",
        "print(\"Conditional Marginals:\", cond_margins)\n",
        "\n",
        "print(\"\\n3. Strong U3 Correlation with U1 and U2\")\n",
        "r1, r2, r3 = generate_extreme_correlation_rankings(100, correlated_var='U3')\n",
        "joint_prob, cond_margins = analyzer.analyze(r1, r2, r3)\n",
        "print(f\"Mean Joint Probability: {joint_prob:.6f}\")\n",
        "print(\"Conditional Marginals:\", cond_margins)\n"
      ]
    }
  ]
}
{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "791dd198-8612-49b3-b802-70d1d9607ac8",
   "metadata": {},
   "source": [
    "# Visuals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b122c59c-7d17-4bd7-b4db-75060a730e81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9a13606fae984358a495b5ce019bd49d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<h3>CDEF Copula Visualization</h3>'), HTML(value='<p><b>Adjust parameters or select…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d433fc67e8f144febac286e2a87b0bf6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Output()"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, clear_output\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "from scipy.stats import kendalltau, gumbel_r\n",
    "from copulas.bivariate import Gumbel\n",
    "import math\n",
    "\n",
    "\n",
    "class RankDependencyAnalyzer:\n",
    "    \"\"\"\n",
    "    Fixed Rank Dependency Analyzer for visualization.\n",
    "    \n",
    "    Properly models:\n",
    "    - Concordance (Kendall's W): Overall agreement\n",
    "    - Concurrence (Mutual Information): Shared information\n",
    "    - Extremeness (Gumbel theta): Tail dependence\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples=10000, random_seed=42):\n",
    "        self.num_samples = num_samples\n",
    "        np.random.seed(random_seed)\n",
    "\n",
    "    def estimate_gumbel_theta(self, rankings):\n",
    "        \"\"\"Estimate Gumbel theta from rankings: theta = 1/(1-tau)\"\"\"\n",
    "        taus = [kendalltau(rankings[i], rankings[j])[0] \n",
    "                for i in range(len(rankings)) \n",
    "                for j in range(i+1, len(rankings))]\n",
    "        tau = np.median(taus)\n",
    "        \n",
    "        # Gumbel constraint: theta >= 1\n",
    "        if tau >= 0.99:\n",
    "            theta = 100.0\n",
    "        elif tau <= 0:\n",
    "            theta = 1.0\n",
    "        else:\n",
    "            theta = 1.0 / (1.0 - tau)\n",
    "        \n",
    "        return np.clip(theta, 1.0, 50.0)\n",
    "\n",
    "    def generate_copula_samples(self, theta, W, MI):\n",
    "        \"\"\"\n",
    "        Generate copula samples representing CDEF components.\n",
    "        \n",
    "        Args:\n",
    "            theta: Gumbel parameter (extremeness/tail dependence)\n",
    "            W: Kendall's W (concordance, 0-1)\n",
    "            MI: Mutual Information (concurrence, 0+)\n",
    "        \n",
    "        Returns:\n",
    "            U1, U2, U3: Three uniform marginals representing the components\n",
    "        \"\"\"\n",
    "        # Component 1: Concordance (Kendall's W)\n",
    "        # Model as correlation between raters - higher W = more correlation\n",
    "        mean = [0, 0]\n",
    "        cov = [[1, W], [W, 1]]  # Correlation = W\n",
    "        X_concordance = np.random.multivariate_normal(mean, cov, self.num_samples)\n",
    "        U1 = stats.norm.cdf(X_concordance[:, 0])\n",
    "        \n",
    "        # Component 2: Concurrence (Mutual Information)\n",
    "        # Model as shared vs independent information\n",
    "        # Higher MI = more shared structure\n",
    "        if MI > 0.5:\n",
    "            # High MI: significant shared information\n",
    "            shared = np.random.uniform(0, 1, self.num_samples)\n",
    "            independent = np.random.uniform(0, 1, self.num_samples)\n",
    "            # Weight by MI (normalized)\n",
    "            MI_weight = np.clip(MI / 3.0, 0, 1)  # Normalize MI to [0,1]\n",
    "            U2 = MI_weight * shared + (1 - MI_weight) * independent\n",
    "        else:\n",
    "            # Low MI: mostly independent\n",
    "            U2 = np.random.uniform(0, 1, self.num_samples)\n",
    "        \n",
    "        # Component 3: Extremeness (Gumbel theta)\n",
    "        # Use actual Gumbel copula for tail dependence\n",
    "        copula = Gumbel()\n",
    "        copula.theta = theta\n",
    "        \n",
    "        # Generate from Gumbel copula (upper-tail dependence)\n",
    "        # Use the copula's sampling method\n",
    "        try:\n",
    "            gumbel_samples = copula.sample(self.num_samples)\n",
    "            U3 = gumbel_samples[:, 0]  # Take first marginal\n",
    "        except:\n",
    "            # Fallback: use inverse transform method\n",
    "            # Gumbel CDF: exp(-((-log u)^theta + (-log v)^theta)^(1/theta))\n",
    "            u_base = np.random.uniform(0.001, 0.999, self.num_samples)\n",
    "            v_base = np.random.uniform(0.001, 0.999, self.num_samples)\n",
    "            \n",
    "            # Apply Gumbel transformation\n",
    "            log_u = -np.log(u_base)\n",
    "            log_v = -np.log(v_base)\n",
    "            U3 = np.exp(-(log_u**theta + log_v**theta)**(1/theta))\n",
    "        \n",
    "        return U1, U2, U3\n",
    "\n",
    "\n",
    "def generate_copula_wireframe(theta, W, MI, N=30):\n",
    "    \"\"\"\n",
    "    Generate wireframe for 3D copula visualization.\n",
    "    \n",
    "    Args:\n",
    "        theta: Gumbel parameter (extremeness)\n",
    "        W: Kendall's W (concordance)\n",
    "        MI: Mutual Information (concurrence)\n",
    "        N: Grid resolution\n",
    "    \"\"\"\n",
    "    analyzer = RankDependencyAnalyzer(num_samples=10000)\n",
    "    U1, U2, U3 = analyzer.generate_copula_samples(theta, W, MI)\n",
    "\n",
    "    # Create grid\n",
    "    u = np.linspace(0.01, 0.99, N)\n",
    "    v = np.linspace(0.01, 0.99, N)\n",
    "    U, V = np.meshgrid(u, v)\n",
    "\n",
    "    # Estimate joint density on grid\n",
    "    U_exp = U[np.newaxis, :, :]\n",
    "    V_exp = V[np.newaxis, :, :]\n",
    "\n",
    "    sample_1_exp = U1[:, np.newaxis, np.newaxis]\n",
    "    sample_2_exp = U2[:, np.newaxis, np.newaxis]\n",
    "\n",
    "    # Empirical copula: proportion of samples <= (u,v)\n",
    "    Z = np.mean((sample_1_exp <= U_exp) & (sample_2_exp <= V_exp), axis=0)\n",
    "    \n",
    "    # Weight by extremeness (U3) to show tail dependence\n",
    "    extremeness_weight = np.mean(U3) if theta > 1.5 else 1.0\n",
    "    Z = Z * extremeness_weight\n",
    "\n",
    "    return U, V, Z\n",
    "\n",
    "\n",
    "# Initial Values (based on your real data)\n",
    "initial_theta = 5.21  # Your data's scaled theta\n",
    "initial_W = 0.85      # Your data's Kendall's W\n",
    "initial_MI = 1.266    # Your data's MI\n",
    "initial_elev = 30\n",
    "initial_azim = 45\n",
    "\n",
    "# Interactive Widgets\n",
    "theta_slider = widgets.FloatSlider(\n",
    "    value=initial_theta, min=1.0, max=30.0, step=0.5, \n",
    "    description='Extremeness (θ)',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "W_slider = widgets.FloatSlider(\n",
    "    value=initial_W, min=0.0, max=1.0, step=0.05, \n",
    "    description='Concordance (W)',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "MI_slider = widgets.FloatSlider(\n",
    "    value=initial_MI, min=0.0, max=3.0, step=0.1, \n",
    "    description='Concurrence (MI)',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "elev_slider = widgets.IntSlider(\n",
    "    value=initial_elev, min=-90, max=90, step=5, \n",
    "    description='Elevation'\n",
    ")\n",
    "azim_slider = widgets.IntSlider(\n",
    "    value=initial_azim, min=0, max=360, step=5, \n",
    "    description='Azimuth'\n",
    ")\n",
    "\n",
    "# Preset buttons for scenarios\n",
    "preset_buttons = widgets.ToggleButtons(\n",
    "    options=[\n",
    "        ('Your Data (Genuine)', (5.21, 0.85, 1.266)),\n",
    "        ('Phantom', (30.7, 0.96, 2.358)),\n",
    "        ('Strong Genuine', (4.2, 0.75, 1.611)),\n",
    "        ('Random', (1.3, 0.27, 0.59))\n",
    "    ],\n",
    "    description='Presets:',\n",
    "    style={'description_width': 'initial'}\n",
    ")\n",
    "\n",
    "\n",
    "def update_plot(theta, W, MI, elev, azim):\n",
    "    \"\"\"Update the 3D wireframe plot\"\"\"\n",
    "    clear_output(wait=True)\n",
    "\n",
    "    fig = plt.figure(figsize=(12, 8))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "\n",
    "    U, V, Z = generate_copula_wireframe(theta, W, MI)\n",
    "\n",
    "    # Create wireframe\n",
    "    ax.plot_wireframe(U, V, Z, color='blue', alpha=0.6, linewidth=0.5)\n",
    "    \n",
    "    # Add surface for better visualization\n",
    "    surf = ax.plot_surface(U, V, Z, cmap='viridis', alpha=0.3, \n",
    "                           edgecolor='none', antialiased=True)\n",
    "    \n",
    "    ax.set_xlabel(\"Concordance (Kendall's W)\", fontsize=10)\n",
    "    ax.set_ylabel(\"Concurrence (Mutual Information)\", fontsize=10)\n",
    "    ax.set_zlabel(\"Joint Density (weighted by θ)\", fontsize=10)\n",
    "    \n",
    "    # Interpret the scenario\n",
    "    if theta > 15 and W > 0.85:\n",
    "        scenario = \"⚠️ PHANTOM: Shared extreme biases\"\n",
    "        color = 'red'\n",
    "    elif W > 0.65 and 2.5 < theta < 6.5:\n",
    "        scenario = \"✓ GENUINE: Natural agreement\"\n",
    "        color = 'green'\n",
    "    elif W < 0.4:\n",
    "        scenario = \"○ RANDOM: No systematic agreement\"\n",
    "        color = 'gray'\n",
    "    else:\n",
    "        scenario = \"→ MIXED: Moderate agreement\"\n",
    "        color = 'orange'\n",
    "    \n",
    "    title = f\"CDEF Copula Visualization\\n\"\n",
    "    title += f\"θ={theta:.2f}, W={W:.2f}, MI={MI:.2f}\\n\"\n",
    "    title += f\"{scenario}\"\n",
    "    \n",
    "    ax.set_title(title, fontsize=12, color=color, weight='bold')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    \n",
    "    # Add colorbar\n",
    "    fig.colorbar(surf, ax=ax, shrink=0.5, aspect=5)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Display relative importance\n",
    "    total = W + MI + theta\n",
    "    print(f\"\\nRelative Importance (NOT probabilities):\")\n",
    "    print(f\"  Concordance (W):  {W/total:.3f} ({W/total*100:.1f}%)\")\n",
    "    print(f\"  Concurrence (MI): {MI/total:.3f} ({MI/total*100:.1f}%)\")\n",
    "    print(f\"  Extremeness (θ):  {theta/total:.3f} ({theta/total*100:.1f}%)\")\n",
    "    print(f\"\\nInterpretation:\")\n",
    "    if theta/total > 0.6:\n",
    "        print(\"  → Tail dependence DOMINATES (agreement on extremes)\")\n",
    "    elif W/total > 0.5:\n",
    "        print(\"  → Global concordance DOMINATES (uniform agreement)\")\n",
    "    else:\n",
    "        print(\"  → Balanced contribution from all factors\")\n",
    "\n",
    "\n",
    "def apply_preset(change):\n",
    "    \"\"\"Apply preset values when button is clicked\"\"\"\n",
    "    theta, W, MI = preset_buttons.value\n",
    "    theta_slider.value = theta\n",
    "    W_slider.value = W\n",
    "    MI_slider.value = MI\n",
    "\n",
    "\n",
    "preset_buttons.observe(apply_preset, names='value')\n",
    "\n",
    "# Display Widgets and Plot\n",
    "ui = widgets.VBox([\n",
    "    widgets.HTML(\"<h3>CDEF Copula Visualization</h3>\"),\n",
    "    widgets.HTML(\"<p><b>Adjust parameters or select presets:</b></p>\"),\n",
    "    preset_buttons,\n",
    "    widgets.HTML(\"<p><b>Manual controls:</b></p>\"),\n",
    "    theta_slider, \n",
    "    W_slider, \n",
    "    MI_slider,\n",
    "    widgets.HTML(\"<p><b>View angle:</b></p>\"),\n",
    "    elev_slider, \n",
    "    azim_slider\n",
    "])\n",
    "\n",
    "output = widgets.interactive_output(update_plot, {\n",
    "    'theta': theta_slider,\n",
    "    'W': W_slider,\n",
    "    'MI': MI_slider,\n",
    "    'elev': elev_slider,\n",
    "    'azim': azim_slider\n",
    "})\n",
    "\n",
    "display(ui, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede5e9c8-19b8-4888-9a8d-9b85c00f3fb3",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "f3447b50-0b30-4156-9f68-c05fab3d363e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Loaded data: 136 ratees x 4 raters\n",
      "Raters: ['CBS', 'CFN', 'Congrove', 'NYT']\n",
      "Ranking type: forced\n",
      "======================================================================\n",
      "GUMBEL COPULA ANALYSIS RESULTS\n",
      "======================================================================\n",
      "\n",
      "Ranking Type: forced\n",
      "Distribution Model: Mallows (forced, dependent)\n",
      "Model Log-Likelihood: -5112.4\n",
      "\n",
      "Core Metrics:\n",
      "  Theta (scaled with W):        5.21\n",
      "  Gumbel theta (from tau):      2.816\n",
      "  Kendall's W (concordance):    0.85\n",
      "  Avg Kendall's tau:            0.645\n",
      "\n",
      "Dependence Tests:\n",
      "  Mutual information:           1.266\n",
      "  Chi-square statistic:         409.897\n",
      "  p-value:                      0.0\n",
      "\n",
      "Log-Likelihoods:\n",
      "  Copula (average):             0.578401\n",
      "  Independence baseline:        0.0\n",
      "\n",
      "Relative Importance (NOT probabilities):\n",
      "  Concordance    : 0.116\n",
      "  Concurrence    : 0.173\n",
      "  Extremeness    : 0.711\n",
      "\n",
      "Pairwise Gumbel Thetas:\n",
      "  CBS-CFN: 5.35\n",
      "  CBS-Congrove: 2.094\n",
      "  CBS-NYT: 5.564\n",
      "  CFN-Congrove: 1.852\n",
      "  CFN-NYT: 3.628\n",
      "  Congrove-NYT: 2.124\n",
      "\n",
      "Kendall's Tau Matrix:\n",
      "Rater       CBS    CFN  Congrove    NYT\n",
      "Rater                                  \n",
      "CBS       1.000  0.813     0.522  0.820\n",
      "CFN       0.813  1.000     0.460  0.724\n",
      "Congrove  0.522  0.460     1.000  0.529\n",
      "NYT       0.820  0.724     0.529  1.000\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Gumbel Copula Analyzer for Ranking Data\n",
    "\n",
    "Handles both forced-choice rankings (strict permutations) and non-forced rankings (ties allowed).\n",
    "Auto-detects ranking type and applies appropriate statistical models.\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kendalltau, chi2_contingency, entropy, multinomial\n",
    "from scipy.spatial.distance import cdist\n",
    "from copulas.bivariate import Gumbel\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from dataclasses import dataclass\n",
    "import warnings\n",
    "import math\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class CopulaResults:\n",
    "    \"\"\"Container for copula analysis results\"\"\"\n",
    "    theta_scaled: float\n",
    "    theta_gumbel: float\n",
    "    kendalls_W: float\n",
    "    avg_kendalls_tau: float\n",
    "    mutual_information: float\n",
    "    chi_square_stat: float\n",
    "    p_value: float\n",
    "    avg_log_likelihood: float\n",
    "    independence_log_likelihood: float\n",
    "    pairwise_thetas: Dict[str, float]\n",
    "    tau_matrix: pd.DataFrame\n",
    "    ranking_type: str\n",
    "    distribution_model: str\n",
    "    model_log_likelihood: Optional[float]\n",
    "    relative_importance: Dict[str, float]\n",
    "    n_raters: int\n",
    "    n_items: int\n",
    "\n",
    "\n",
    "class RankDependencyAnalyzer:\n",
    "    \"\"\"\n",
    "    Analyze ranking dependence using Gumbel copulas.\n",
    "    \n",
    "    Features:\n",
    "    - Auto-detects forced vs non-forced rankings\n",
    "    - Applies appropriate statistical models (Mallows, Multinomial, etc.)\n",
    "    - Computes dependence via Gumbel copulas (upper-tail dependence)\n",
    "    - Provides interpretable metrics (Kendall's W, tau, theta)\n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, num_samples: int = 10000, significance_level: float = 0.05, \n",
    "                 random_seed: Optional[int] = 42):\n",
    "        \"\"\"\n",
    "        Initialize analyzer.\n",
    "        \n",
    "        Args:\n",
    "            num_samples: Number of Monte Carlo samples for permutation tests\n",
    "            significance_level: Alpha level for hypothesis tests\n",
    "            random_seed: Random seed for reproducibility\n",
    "        \"\"\"\n",
    "        self.num_samples = num_samples\n",
    "        self.significance_level = significance_level\n",
    "        self.random_seed = random_seed\n",
    "        if random_seed is not None:\n",
    "            np.random.seed(random_seed)\n",
    "        \n",
    "        self.copulas: Dict[str, Gumbel] = {}\n",
    "        self.theta: Optional[float] = None\n",
    "        self.W: Optional[float] = None\n",
    "        \n",
    "    def load_excel(self, file_path: str, sheet_name: str, \n",
    "                   rater_col: str, ratee_col: str, ranking_col: str) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Load rankings from Excel in long format, convert to wide format.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to Excel file\n",
    "            sheet_name: Sheet name\n",
    "            rater_col: Column name for raters\n",
    "            ratee_col: Column name for items being ranked\n",
    "            ranking_col: Column name for rank values\n",
    "            \n",
    "        Returns:\n",
    "            DataFrame with ratees as rows, raters as columns\n",
    "            \n",
    "        Raises:\n",
    "            ValueError: If required columns are missing\n",
    "            FileNotFoundError: If file doesn't exist\n",
    "        \"\"\"\n",
    "        try:\n",
    "            df = pd.read_excel(file_path, sheet_name=sheet_name)\n",
    "        except FileNotFoundError as e:\n",
    "            raise FileNotFoundError(f\"Excel file not found: {file_path}\") from e\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"Error reading Excel file: {e}\") from e\n",
    "        \n",
    "        # Validate columns\n",
    "        required_cols = [rater_col, ratee_col, ranking_col]\n",
    "        missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "        if missing_cols:\n",
    "            raise ValueError(\n",
    "                f\"Missing columns: {missing_cols}. \"\n",
    "                f\"Available columns: {df.columns.tolist()}\"\n",
    "            )\n",
    "        \n",
    "        # Pivot to wide format\n",
    "        rankings_wide = df.pivot(index=ratee_col, columns=rater_col, values=ranking_col)\n",
    "        \n",
    "        # Drop rows with missing values\n",
    "        n_before = len(rankings_wide)\n",
    "        rankings_wide = rankings_wide.dropna()\n",
    "        n_after = len(rankings_wide)\n",
    "        \n",
    "        if n_after < n_before:\n",
    "            warnings.warn(f\"Dropped {n_before - n_after} rows with missing values\")\n",
    "        \n",
    "        if n_after == 0:\n",
    "            raise ValueError(\"No complete rankings found after dropping missing values\")\n",
    "        \n",
    "        return rankings_wide.astype(int)\n",
    "    \n",
    "    def detect_ranking_type(self, rankings_df: pd.DataFrame) -> str:\n",
    "        \"\"\"\n",
    "        Detect if rankings are forced-choice (permutation) or non-forced (ties allowed).\n",
    "        \n",
    "        Args:\n",
    "            rankings_df: Rankings with ratees as rows, raters as columns\n",
    "            \n",
    "        Returns:\n",
    "            'forced' if strict permutations, 'non-forced' if ties exist\n",
    "        \"\"\"\n",
    "        n_items = len(rankings_df)\n",
    "        \n",
    "        for col in rankings_df.columns:\n",
    "            ranks = rankings_df[col]\n",
    "            \n",
    "            # Check for duplicate ranks (ties)\n",
    "            if len(ranks) != len(ranks.unique()):\n",
    "                return 'non-forced'\n",
    "            \n",
    "            # Check if it's a complete permutation [1, 2, ..., n]\n",
    "            expected_ranks = set(range(1, n_items + 1))\n",
    "            if set(ranks) != expected_ranks:\n",
    "                return 'non-forced'\n",
    "        \n",
    "        return 'forced'\n",
    "    \n",
    "    def compute_kendalls_W(self, rankings_df: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Compute Kendall's W (coefficient of concordance).\n",
    "        \n",
    "        W = 1 indicates perfect agreement, W = 0 indicates no agreement.\n",
    "        \n",
    "        Args:\n",
    "            rankings_df: Rankings with ratees as rows, raters as columns\n",
    "            \n",
    "        Returns:\n",
    "            Kendall's W\n",
    "        \"\"\"\n",
    "        N, m = rankings_df.shape  # N items, m raters\n",
    "        row_sums = rankings_df.sum(axis=1)\n",
    "        mean_rank_sum = row_sums.mean()\n",
    "        S = np.sum((row_sums - mean_rank_sum) ** 2)\n",
    "        W = (12 * S) / (m ** 2 * (N ** 3 - N))\n",
    "        return round(W, 3)\n",
    "    \n",
    "    def compute_mutual_information_and_independence(\n",
    "        self, rankings1: np.ndarray, rankings2: np.ndarray\n",
    "    ) -> Tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Compute mutual information and chi-square test for independence.\n",
    "        \n",
    "        Uses binning to create contingency table for chi-square test.\n",
    "        \n",
    "        Args:\n",
    "            rankings1: First rater's rankings\n",
    "            rankings2: Second rater's rankings\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (mutual_information, p_value, chi_square_statistic)\n",
    "        \"\"\"\n",
    "        # Determine bin count (sqrt rule, minimum 5 for chi-square validity)\n",
    "        bins = max(int(np.ceil(np.sqrt(len(rankings1)))), 5)\n",
    "        \n",
    "        # Create 2D histogram (contingency table)\n",
    "        joint_dist, _, _ = np.histogram2d(\n",
    "            rankings1, rankings2, \n",
    "            bins=bins,\n",
    "            range=[[rankings1.min(), rankings1.max()], \n",
    "                   [rankings2.min(), rankings2.max()]]\n",
    "        )\n",
    "        \n",
    "        # Chi-square test for independence\n",
    "        chi2, p_value, dof, _ = chi2_contingency(joint_dist)\n",
    "        \n",
    "        # Compute mutual information\n",
    "        # Add small constant to avoid log(0)\n",
    "        joint_dist_smooth = joint_dist + 1e-10\n",
    "        joint_dist_norm = joint_dist_smooth / np.sum(joint_dist_smooth)\n",
    "        \n",
    "        marginal_x = np.sum(joint_dist_norm, axis=1)\n",
    "        marginal_y = np.sum(joint_dist_norm, axis=0)\n",
    "        \n",
    "        mi = (entropy(marginal_x) + entropy(marginal_y) - \n",
    "              entropy(joint_dist_norm.flatten()))\n",
    "        \n",
    "        return round(mi, 3), round(p_value, 3), round(chi2, 3)\n",
    "    \n",
    "    def estimate_gumbel_theta(self, rankings_df: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Estimate Gumbel copula parameter from Kendall's tau.\n",
    "        \n",
    "        For Gumbel copula: θ = 1/(1-τ), where τ is Kendall's tau.\n",
    "        θ ≥ 1, with θ=1 indicating independence.\n",
    "        \n",
    "        Args:\n",
    "            rankings_df: Rankings with ratees as rows, raters as columns\n",
    "            \n",
    "        Returns:\n",
    "            Gumbel theta parameter\n",
    "        \"\"\"\n",
    "        raters = rankings_df.columns\n",
    "        taus = []\n",
    "        \n",
    "        for i, r1 in enumerate(raters):\n",
    "            for r2 in raters[i + 1:]:\n",
    "                tau, _ = kendalltau(rankings_df[r1], rankings_df[r2])\n",
    "                taus.append(tau)\n",
    "        \n",
    "        avg_tau = np.mean(taus) if taus else 0\n",
    "        \n",
    "        # Gumbel constraint: theta >= 1\n",
    "        if avg_tau >= 0.99:\n",
    "            theta = 100.0  # Cap at high value\n",
    "        elif avg_tau <= 0:\n",
    "            theta = 1.0  # Independence\n",
    "        else:\n",
    "            theta = 1.0 / (1.0 - avg_tau)\n",
    "        \n",
    "        return round(theta, 3)\n",
    "    \n",
    "    def estimate_copula_theta(self, rankings_df: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Estimate scaled theta incorporating both pairwise (tau) and global (W) agreement.\n",
    "        \n",
    "        This combines local pairwise dependence with global concordance.\n",
    "        \n",
    "        Args:\n",
    "            rankings_df: Rankings with ratees as rows, raters as columns\n",
    "            \n",
    "        Returns:\n",
    "            Scaled theta parameter\n",
    "        \"\"\"\n",
    "        theta_gumbel = self.estimate_gumbel_theta(rankings_df)\n",
    "        self.W = self.compute_kendalls_W(rankings_df)\n",
    "        \n",
    "        # Scale theta by (1 + W) to incorporate global concordance\n",
    "        self.theta = round(theta_gumbel * (1 + self.W), 3)\n",
    "        return self.theta\n",
    "    \n",
    "    def fit_gumbel_copulas(self, rankings_df: pd.DataFrame) -> None:\n",
    "        \"\"\"\n",
    "        Fit bivariate Gumbel copulas for all rater pairs.\n",
    "        \n",
    "        Transforms rankings to uniform [0,1] marginals via empirical CDF,\n",
    "        then fits Gumbel copula to capture upper-tail dependence.\n",
    "        \n",
    "        Args:\n",
    "            rankings_df: Rankings with ratees as rows, raters as columns\n",
    "        \"\"\"\n",
    "        raters = rankings_df.columns\n",
    "        n = len(rankings_df)\n",
    "        \n",
    "        for i, r1 in enumerate(raters):\n",
    "            for r2 in raters[i+1:]:\n",
    "                try:\n",
    "                    copula = Gumbel()\n",
    "                    \n",
    "                    # Transform to uniform [0,1] using empirical CDF\n",
    "                    u1 = (rankings_df[r1].rank() - 0.5) / n\n",
    "                    u2 = (rankings_df[r2].rank() - 0.5) / n\n",
    "                    data = np.column_stack([u1.values, u2.values])\n",
    "                    \n",
    "                    copula.fit(data)\n",
    "                    self.copulas[f\"{r1}-{r2}\"] = copula\n",
    "                    \n",
    "                except ValueError as e:\n",
    "                    # Gumbel requires theta >= 1 (positive dependence only)\n",
    "                    # If fit fails, use independence (theta=1)\n",
    "                    warnings.warn(\n",
    "                        f\"Could not fit Gumbel copula for {r1}-{r2}: {e}. \"\n",
    "                        f\"Using independence (θ=1).\"\n",
    "                    )\n",
    "                    copula = Gumbel()\n",
    "                    copula.theta = 1.0\n",
    "                    self.copulas[f\"{r1}-{r2}\"] = copula\n",
    "    \n",
    "    def compute_avg_log_likelihood(self, rankings_df: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Compute average log-likelihood across all observations under fitted copulas.\n",
    "        \n",
    "        This properly uses all data points, not just the mean.\n",
    "        \n",
    "        Args:\n",
    "            rankings_df: Rankings with ratees as rows, raters as columns\n",
    "            \n",
    "        Returns:\n",
    "            Average log-likelihood per observation\n",
    "        \"\"\"\n",
    "        if not self.copulas:\n",
    "            return 0.0\n",
    "        \n",
    "        n = len(rankings_df)\n",
    "        log_likelihoods = []\n",
    "        \n",
    "        for pair_name, copula in self.copulas.items():\n",
    "            r1, r2 = pair_name.split('-')\n",
    "            \n",
    "            # Transform to uniform [0,1]\n",
    "            u1 = (rankings_df[r1].rank() - 0.5) / n\n",
    "            u2 = (rankings_df[r2].rank() - 0.5) / n\n",
    "            \n",
    "            # Compute log-likelihood for each observation\n",
    "            for i in range(n):\n",
    "                point = np.array([[u1.iloc[i], u2.iloc[i]]])\n",
    "                try:\n",
    "                    density = copula.probability_density(point)\n",
    "                    if density > 0:\n",
    "                        log_likelihoods.append(np.log(density))\n",
    "                except (ValueError, RuntimeError):\n",
    "                    continue\n",
    "        \n",
    "        if not log_likelihoods:\n",
    "            return 0.0\n",
    "        \n",
    "        return round(np.mean(log_likelihoods), 6)\n",
    "    \n",
    "    def compute_independence_log_likelihood(self, rankings_df: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Compute log-likelihood under independence (uniform copula).\n",
    "        \n",
    "        Under independence, copula density = 1 everywhere, so log-likelihood = 0.\n",
    "        \n",
    "        Returns:\n",
    "            0.0 (independence baseline)\n",
    "        \"\"\"\n",
    "        return 0.0\n",
    "    \n",
    "    def compute_consensus_ranking(self, rankings_df: pd.DataFrame) -> pd.Series:\n",
    "        \"\"\"\n",
    "        Compute consensus ranking using Borda count.\n",
    "        \n",
    "        Lower sum of ranks = better overall ranking.\n",
    "        \n",
    "        Args:\n",
    "            rankings_df: Rankings with ratees as rows, raters as columns\n",
    "            \n",
    "        Returns:\n",
    "            Consensus ranking (1 = best)\n",
    "        \"\"\"\n",
    "        borda_scores = rankings_df.sum(axis=1)\n",
    "        consensus = borda_scores.rank(method='min').astype(int)\n",
    "        return consensus\n",
    "    \n",
    "    def compute_kendall_distance(self, rank1: np.ndarray, rank2: np.ndarray) -> int:\n",
    "        \"\"\"\n",
    "        Compute Kendall tau distance (number of pairwise disagreements).\n",
    "        \n",
    "        Args:\n",
    "            rank1: First ranking\n",
    "            rank2: Second ranking\n",
    "            \n",
    "        Returns:\n",
    "            Number of discordant pairs\n",
    "        \"\"\"\n",
    "        n = len(rank1)\n",
    "        distance = 0\n",
    "        \n",
    "        for i in range(n):\n",
    "            for j in range(i + 1, n):\n",
    "                # Check if pair (i,j) is ordered differently in the two rankings\n",
    "                if (rank1[i] < rank1[j]) != (rank2[i] < rank2[j]):\n",
    "                    distance += 1\n",
    "        \n",
    "        return distance\n",
    "    \n",
    "    def fit_mallows_model(self, rankings_df: pd.DataFrame) -> Dict:\n",
    "        \"\"\"\n",
    "        Fit Mallows model for forced rankings under dependence.\n",
    "        \n",
    "        Mallows model: P(σ) ∝ exp(-θ * d(σ, σ₀))\n",
    "        where d is Kendall distance and σ₀ is consensus ranking.\n",
    "        \n",
    "        Args:\n",
    "            rankings_df: Rankings with ratees as rows, raters as columns\n",
    "            \n",
    "        Returns:\n",
    "            Dict with 'theta' (dispersion), 'consensus' ranking, and log-likelihood\n",
    "        \"\"\"\n",
    "        # Compute consensus ranking\n",
    "        consensus = self.compute_consensus_ranking(rankings_df)\n",
    "        \n",
    "        # Compute average Kendall distance to consensus\n",
    "        distances = []\n",
    "        for col in rankings_df.columns:\n",
    "            d = self.compute_kendall_distance(\n",
    "                rankings_df[col].values,\n",
    "                consensus.values\n",
    "            )\n",
    "            distances.append(d)\n",
    "        \n",
    "        avg_distance = np.mean(distances)\n",
    "        n = len(rankings_df)\n",
    "        \n",
    "        # Estimate theta (simple MLE approximation)\n",
    "        # For Mallows: E[d] ≈ n(n-1)/(4 * (1 + exp(θ)))\n",
    "        # Rough inverse: theta ≈ log((n(n-1))/(4*avg_distance) - 1)\n",
    "        if avg_distance > 0:\n",
    "            theta_mallows = max(0.01, np.log(max(n * (n-1) / (4 * avg_distance) - 1, 1.01)))\n",
    "        else:\n",
    "            theta_mallows = 10.0  # Perfect agreement\n",
    "        \n",
    "        # Compute log-likelihood (approximate)\n",
    "        # L = -θ * Σd(σᵢ, σ₀) - log(Z(θ))\n",
    "        # Z(θ) is partition function (expensive to compute exactly, use approximation)\n",
    "        log_likelihood = -theta_mallows * sum(distances)\n",
    "        \n",
    "        return {\n",
    "            'theta': round(theta_mallows, 3),\n",
    "            'consensus': consensus,\n",
    "            'avg_distance': round(avg_distance, 2),\n",
    "            'log_likelihood': round(log_likelihood, 3)\n",
    "        }\n",
    "    \n",
    "    def calculate_multinomial_log_likelihood(self, rankings_df: pd.DataFrame) -> float:\n",
    "        \"\"\"\n",
    "        Compute log-likelihood under multinomial model (independence baseline).\n",
    "        \n",
    "        Treats each (team, rank) pair as independent multinomial draw.\n",
    "        \n",
    "        Args:\n",
    "            rankings_df: Rankings with ratees as rows, raters as columns\n",
    "            \n",
    "        Returns:\n",
    "            Log-likelihood under multinomial model\n",
    "        \"\"\"\n",
    "        # Flatten to get all (team, rank) pairs\n",
    "        all_pairs = []\n",
    "        for col in rankings_df.columns:\n",
    "            for idx, rank in rankings_df[col].items():\n",
    "                all_pairs.append((idx, rank))\n",
    "        \n",
    "        # Count frequencies\n",
    "        unique_pairs, counts = np.unique(all_pairs, axis=0, return_counts=True)\n",
    "        \n",
    "        # Estimate probabilities (MLE)\n",
    "        n_total = len(all_pairs)\n",
    "        probs = counts / n_total\n",
    "        \n",
    "        # Multinomial log-likelihood\n",
    "        log_likelihood = np.sum(counts * np.log(probs + 1e-10))\n",
    "        \n",
    "        return round(log_likelihood, 3)\n",
    "    \n",
    "    def choose_distribution_model(self, rankings_df: pd.DataFrame) -> Tuple[str, Optional[float]]:\n",
    "        \"\"\"\n",
    "        Choose appropriate distribution based on ranking type and dependence test.\n",
    "        \n",
    "        Logic:\n",
    "        - Forced + Dependent → Mallows model\n",
    "        - Forced + Independent → Uniform permutation\n",
    "        - Non-forced + Dependent → Report dependence (use copula)\n",
    "        - Non-forced + Independent → Multinomial\n",
    "        \n",
    "        Args:\n",
    "            rankings_df: Rankings with ratees as rows, raters as columns\n",
    "            \n",
    "        Returns:\n",
    "            Tuple of (model_name, log_likelihood)\n",
    "        \"\"\"\n",
    "        ranking_type = self.detect_ranking_type(rankings_df)\n",
    "        \n",
    "        # Test for dependence\n",
    "        col1, col2 = rankings_df.columns[0], rankings_df.columns[1]\n",
    "        _, p_value, _ = self.compute_mutual_information_and_independence(\n",
    "            rankings_df[col1].values, rankings_df[col2].values\n",
    "        )\n",
    "        \n",
    "        is_dependent = p_value < self.significance_level\n",
    "        \n",
    "        if ranking_type == 'forced':\n",
    "            if is_dependent:\n",
    "                model_result = self.fit_mallows_model(rankings_df)\n",
    "                return 'Mallows (forced, dependent)', model_result['log_likelihood']\n",
    "            else:\n",
    "                # Uniform over all permutations\n",
    "                # For large n, use Stirling approximation: log(n!) ≈ n*log(n) - n\n",
    "                n = len(rankings_df)\n",
    "                if n > 170:  # factorial(171) overflows\n",
    "                    log_factorial_n = n * np.log(n) - n  # Stirling\n",
    "                else:\n",
    "                    log_factorial_n = np.log(math.factorial(n))\n",
    "                log_likelihood = -log_factorial_n  # log(1/n!)\n",
    "                return 'Uniform Permutation (forced, independent)', log_likelihood\n",
    "        else:  # non-forced\n",
    "            if is_dependent:\n",
    "                # Just report that dependence detected, rely on copula\n",
    "                return 'Dependent (non-forced, use copula)', None\n",
    "            else:\n",
    "                log_likelihood = self.calculate_multinomial_log_likelihood(rankings_df)\n",
    "                return 'Multinomial (non-forced, independent)', log_likelihood\n",
    "    \n",
    "    def compute_relative_importance(self, W: float, mi: float, theta: float) -> Dict[str, float]:\n",
    "        \"\"\"\n",
    "        Compute relative importance of three factors: Concordance, Concurrence, Extremeness.\n",
    "        \n",
    "        These are NOT conditional probabilities - they are normalized importance weights\n",
    "        showing the relative contribution of each factor.\n",
    "        \n",
    "        Args:\n",
    "            W: Kendall's W (concordance)\n",
    "            mi: Mutual information (concurrence)\n",
    "            theta: Copula parameter (extremeness/tail dependence)\n",
    "            \n",
    "        Returns:\n",
    "            Dict with relative importance weights (sum to 1.0)\n",
    "        \"\"\"\n",
    "        total = W + mi + theta\n",
    "        \n",
    "        if total == 0:\n",
    "            return {\n",
    "                'Concordance': 0.333,\n",
    "                'Concurrence': 0.333,\n",
    "                'Extremeness': 0.333\n",
    "            }\n",
    "        \n",
    "        return {\n",
    "            'Concordance': round(W / total, 3),\n",
    "            'Concurrence': round(mi / total, 3),\n",
    "            'Extremeness': round(theta / total, 3)\n",
    "        }\n",
    "    \n",
    "    def analyze_from_excel(\n",
    "        self, file_path: str, sheet_name: str, \n",
    "        rater_col: str, ratee_col: str, ranking_col: str\n",
    "    ) -> CopulaResults:\n",
    "        \"\"\"\n",
    "        Complete analysis pipeline from Excel file.\n",
    "        \n",
    "        Args:\n",
    "            file_path: Path to Excel file\n",
    "            sheet_name: Sheet name\n",
    "            rater_col: Column name for raters\n",
    "            ratee_col: Column name for items being ranked\n",
    "            ranking_col: Column name for rank values\n",
    "            \n",
    "        Returns:\n",
    "            CopulaResults dataclass with all analysis outputs\n",
    "        \"\"\"\n",
    "        # Load data\n",
    "        rankings_wide = self.load_excel(file_path, sheet_name, rater_col, ratee_col, ranking_col)\n",
    "        \n",
    "        print(f\"\\nLoaded data: {len(rankings_wide)} ratees x {len(rankings_wide.columns)} raters\")\n",
    "        print(f\"Raters: {list(rankings_wide.columns)}\")\n",
    "        \n",
    "        # Detect ranking type\n",
    "        ranking_type = self.detect_ranking_type(rankings_wide)\n",
    "        print(f\"Ranking type: {ranking_type}\")\n",
    "        \n",
    "        # Fit Gumbel copulas (pairwise)\n",
    "        self.fit_gumbel_copulas(rankings_wide)\n",
    "        \n",
    "        # Calculate core metrics\n",
    "        theta_scaled = self.estimate_copula_theta(rankings_wide)\n",
    "        theta_gumbel = self.estimate_gumbel_theta(rankings_wide)\n",
    "        \n",
    "        # Mutual information and chi-square test\n",
    "        col1, col2 = rankings_wide.columns[0], rankings_wide.columns[1]\n",
    "        mi, p_value, chi2_stat = self.compute_mutual_information_and_independence(\n",
    "            rankings_wide[col1].values, rankings_wide[col2].values\n",
    "        )\n",
    "        \n",
    "        # Distribution model selection\n",
    "        distribution_model, model_log_likelihood = self.choose_distribution_model(rankings_wide)\n",
    "        \n",
    "        # Average log-likelihood from copulas\n",
    "        avg_log_likelihood = self.compute_avg_log_likelihood(rankings_wide)\n",
    "        \n",
    "        # Independence baseline\n",
    "        independence_log_likelihood = self.compute_independence_log_likelihood(rankings_wide)\n",
    "        \n",
    "        # Relative importance (not conditional probabilities)\n",
    "        relative_importance = self.compute_relative_importance(self.W, mi, theta_scaled)\n",
    "        \n",
    "        # Tau statistics\n",
    "        taus = []\n",
    "        for i, c1 in enumerate(rankings_wide.columns):\n",
    "            for c2 in rankings_wide.columns[i+1:]:\n",
    "                tau, _ = kendalltau(rankings_wide[c1], rankings_wide[c2])\n",
    "                taus.append(tau)\n",
    "        avg_tau = np.mean(taus) if taus else 0\n",
    "        \n",
    "        # Pairwise Gumbel thetas\n",
    "        pairwise_thetas = {}\n",
    "        for pair_name, copula in self.copulas.items():\n",
    "            pairwise_thetas[pair_name] = round(copula.theta, 3)\n",
    "        \n",
    "        # Tau matrix\n",
    "        tau_matrix = pd.DataFrame(\n",
    "            index=rankings_wide.columns, \n",
    "            columns=rankings_wide.columns,\n",
    "            dtype=float\n",
    "        )\n",
    "        for c1 in rankings_wide.columns:\n",
    "            for c2 in rankings_wide.columns:\n",
    "                if c1 == c2:\n",
    "                    tau_matrix.loc[c1, c2] = 1.0\n",
    "                else:\n",
    "                    tau, _ = kendalltau(rankings_wide[c1], rankings_wide[c2])\n",
    "                    tau_matrix.loc[c1, c2] = round(tau, 3)\n",
    "        \n",
    "        return CopulaResults(\n",
    "            theta_scaled=theta_scaled,\n",
    "            theta_gumbel=theta_gumbel,\n",
    "            kendalls_W=self.W,\n",
    "            avg_kendalls_tau=round(avg_tau, 3),\n",
    "            mutual_information=mi,\n",
    "            chi_square_stat=chi2_stat,\n",
    "            p_value=p_value,\n",
    "            avg_log_likelihood=avg_log_likelihood,\n",
    "            independence_log_likelihood=independence_log_likelihood,\n",
    "            pairwise_thetas=pairwise_thetas,\n",
    "            tau_matrix=tau_matrix,\n",
    "            ranking_type=ranking_type,\n",
    "            distribution_model=distribution_model,\n",
    "            model_log_likelihood=model_log_likelihood,\n",
    "            relative_importance=relative_importance,\n",
    "            n_raters=len(rankings_wide.columns),\n",
    "            n_items=len(rankings_wide)\n",
    "        )\n",
    "\n",
    "\n",
    "def format_results(results: CopulaResults) -> str:\n",
    "    \"\"\"\n",
    "    Format results for display.\n",
    "    \n",
    "    Args:\n",
    "        results: CopulaResults dataclass\n",
    "        \n",
    "    Returns:\n",
    "        Formatted string for printing\n",
    "    \"\"\"\n",
    "    output = []\n",
    "    output.append(\"=\" * 70)\n",
    "    output.append(\"GUMBEL COPULA ANALYSIS RESULTS\")\n",
    "    output.append(\"=\" * 70)\n",
    "    \n",
    "    output.append(f\"\\nRanking Type: {results.ranking_type}\")\n",
    "    output.append(f\"Distribution Model: {results.distribution_model}\")\n",
    "    if results.model_log_likelihood is not None:\n",
    "        output.append(f\"Model Log-Likelihood: {results.model_log_likelihood}\")\n",
    "    \n",
    "    output.append(f\"\\nCore Metrics:\")\n",
    "    output.append(f\"  Theta (scaled with W):        {results.theta_scaled}\")\n",
    "    output.append(f\"  Gumbel theta (from tau):      {results.theta_gumbel}\")\n",
    "    output.append(f\"  Kendall's W (concordance):    {results.kendalls_W}\")\n",
    "    output.append(f\"  Avg Kendall's tau:            {results.avg_kendalls_tau}\")\n",
    "    \n",
    "    output.append(f\"\\nDependence Tests:\")\n",
    "    output.append(f\"  Mutual information:           {results.mutual_information}\")\n",
    "    output.append(f\"  Chi-square statistic:         {results.chi_square_stat}\")\n",
    "    output.append(f\"  p-value:                      {results.p_value}\")\n",
    "    \n",
    "    output.append(f\"\\nLog-Likelihoods:\")\n",
    "    output.append(f\"  Copula (average):             {results.avg_log_likelihood}\")\n",
    "    output.append(f\"  Independence baseline:        {results.independence_log_likelihood}\")\n",
    "    \n",
    "    output.append(f\"\\nRelative Importance (NOT probabilities):\")\n",
    "    for key, val in results.relative_importance.items():\n",
    "        output.append(f\"  {key:15s}: {val:.3f}\")\n",
    "    \n",
    "    output.append(f\"\\nPairwise Gumbel Thetas:\")\n",
    "    for pair, theta_val in results.pairwise_thetas.items():\n",
    "        output.append(f\"  {pair}: {theta_val}\")\n",
    "    \n",
    "    output.append(f\"\\nKendall's Tau Matrix:\")\n",
    "    output.append(str(results.tau_matrix))\n",
    "    \n",
    "    return \"\\n\".join(output)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    analyzer = RankDependencyAnalyzer(random_seed=42)\n",
    "    \n",
    "    file_path = \"/mnt/c/Users/lfult/OneDrive - bc.edu/Desktop/Copula/data.xlsx\"\n",
    "    \n",
    "    try:\n",
    "        results = analyzer.analyze_from_excel(\n",
    "            file_path, \"Sheet1\", \"Rater\", \"Ratee\", \"Ranking\"\n",
    "        )\n",
    "        \n",
    "        print(format_results(results))\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7077e2c-c9be-4b63-bc5f-15c6b8a050c4",
   "metadata": {},
   "source": [
    "# Sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "783ebf5c-7202-42f0-bd4a-f9a7876f36e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "CDEF DEMONSTRATION: Detecting Phantom vs Genuine Concordance\n",
      "Using Properly Fixed Gumbel Copula Analysis\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "PHANTOM (EXTREME BIAS)\n",
      "================================================================================\n",
      "  Saved to: scenario_phantom_extreme_bias.xlsx\n",
      "\n",
      "Loaded data: 136 ratees x 4 raters\n",
      "Raters: ['CBS', 'CFN', 'Congrove', 'NYT']\n",
      "Ranking type: forced\n",
      "\n",
      "Ranking Type: forced\n",
      "Distribution Model: Mallows (forced, dependent)\n",
      "Model Log-Likelihood: -2499.485\n",
      "\n",
      "Core Metrics:\n",
      "  Kendall's W (concordance):     0.964\n",
      "  Theta (scaled):                30.733\n",
      "  Gumbel theta (from tau):       15.648\n",
      "  Avg Kendall's tau:             0.936\n",
      "  Mutual information:            2.358\n",
      "\n",
      "Log-Likelihoods:\n",
      "  Copula (average):              2.047156\n",
      "  Independence baseline:         0.000000\n",
      "\n",
      "Relative Importance (NOT probabilities):\n",
      "  Concordance    : 0.028\n",
      "  Concurrence    : 0.069\n",
      "  Extremeness    : 0.902\n",
      "\n",
      "Pairwise Theta Range:\n",
      "  Max: 35.308, Min: 9.850\n",
      "\n",
      "Traditional Analysis (W only):\n",
      "  High concordance → Good agreement\n",
      "\n",
      "CDEF Analysis (W + θ + MI + τ):\n",
      "  P(Genuine|Data) = 0.100\n",
      "  Interpretation: ⚠️ PHANTOM: Shared extreme biases (very high W + very high θ)\n",
      "\n",
      "================================================================================\n",
      "GENUINE (NATURAL AGREEMENT)\n",
      "================================================================================\n",
      "  Saved to: scenario_genuine_natural_agreement.xlsx\n",
      "\n",
      "Loaded data: 136 ratees x 4 raters\n",
      "Raters: ['CBS', 'CFN', 'Congrove', 'NYT']\n",
      "Ranking type: forced\n",
      "\n",
      "Ranking Type: forced\n",
      "Distribution Model: Mallows (forced, dependent)\n",
      "Model Log-Likelihood: -4838.496\n",
      "\n",
      "Core Metrics:\n",
      "  Kendall's W (concordance):     0.748\n",
      "  Theta (scaled):                4.232\n",
      "  Gumbel theta (from tau):       2.421\n",
      "  Avg Kendall's tau:             0.587\n",
      "  Mutual information:            1.611\n",
      "\n",
      "Log-Likelihoods:\n",
      "  Copula (average):              0.411599\n",
      "  Independence baseline:         0.000000\n",
      "\n",
      "Relative Importance (NOT probabilities):\n",
      "  Concordance    : 0.113\n",
      "  Concurrence    : 0.244\n",
      "  Extremeness    : 0.642\n",
      "\n",
      "Pairwise Theta Range:\n",
      "  Max: 2.830, Min: 2.171\n",
      "\n",
      "Traditional Analysis (W only):\n",
      "  High concordance → Good agreement\n",
      "\n",
      "CDEF Analysis (W + θ + MI + τ):\n",
      "  P(Genuine|Data) = 0.750\n",
      "  Interpretation: ✓ GENUINE: Natural agreement (high W + moderate θ)\n",
      "\n",
      "================================================================================\n",
      "RANDOM (NO AGREEMENT)\n",
      "================================================================================\n",
      "  Saved to: scenario_random_no_agreement.xlsx\n",
      "\n",
      "Loaded data: 136 ratees x 4 raters\n",
      "Raters: ['CBS', 'CFN', 'Congrove', 'NYT']\n",
      "Ranking type: forced\n",
      "Error analyzing Random (No Agreement): loop of ufunc does not support argument 0 of type int which has no callable log method\n",
      "\n",
      "================================================================================\n",
      "CLUSTERED (OUTLIER)\n",
      "================================================================================\n",
      "  Saved to: scenario_clustered_outlier.xlsx\n",
      "\n",
      "Loaded data: 136 ratees x 4 raters\n",
      "Raters: ['CBS', 'CFN', 'Congrove', 'NYT']\n",
      "Ranking type: forced\n",
      "\n",
      "Ranking Type: forced\n",
      "Distribution Model: Mallows (forced, dependent)\n",
      "Model Log-Likelihood: -4571.735\n",
      "\n",
      "Core Metrics:\n",
      "  Kendall's W (concordance):     0.718\n",
      "  Theta (scaled):                3.805\n",
      "  Gumbel theta (from tau):       2.215\n",
      "  Avg Kendall's tau:             0.549\n",
      "  Mutual information:            1.811\n",
      "\n",
      "Log-Likelihoods:\n",
      "  Copula (average):              0.452280\n",
      "  Independence baseline:         0.000000\n",
      "\n",
      "Relative Importance (NOT probabilities):\n",
      "  Concordance    : 0.113\n",
      "  Concurrence    : 0.286\n",
      "  Extremeness    : 0.601\n",
      "\n",
      "Pairwise Theta Range:\n",
      "  Max: 3.877, Min: 1.542\n",
      "\n",
      "Traditional Analysis (W only):\n",
      "  High concordance → Good agreement\n",
      "\n",
      "CDEF Analysis (W + θ + MI + τ):\n",
      "  P(Genuine|Data) = 0.750\n",
      "  Interpretation: ✓ GENUINE: Natural agreement (high W + moderate θ)\n",
      "\n",
      "================================================================================\n",
      "SUMMARY: Traditional vs CDEF Analysis\n",
      "================================================================================\n",
      "\n",
      "Traditional Analysis (Kendall's W only):\n",
      "  Phantom (Extreme Bias)        : W=0.964 → High concordance → Good agreement\n",
      "  Genuine (Natural Agreement)   : W=0.748 → High concordance → Good agreement\n",
      "  Clustered (Outlier)           : W=0.718 → High concordance → Good agreement\n",
      "\n",
      "CDEF Copula Analysis (Full Dependency Structure):\n",
      "\n",
      "  Phantom (Extreme Bias)        :\n",
      "    Type: forced, Model: Mallows (forced, dependent)\n",
      "    W=0.964, θ=30.733, MI=2.358, τ=0.936\n",
      "    Relative Importance: Conc=0.028, Concur=0.069, Extreme=0.902\n",
      "    → P(Genuine|Data) = 0.100\n",
      "    → ⚠️ PHANTOM: Shared extreme biases (very high W + very high θ)\n",
      "\n",
      "  Genuine (Natural Agreement)   :\n",
      "    Type: forced, Model: Mallows (forced, dependent)\n",
      "    W=0.748, θ=4.232, MI=1.611, τ=0.587\n",
      "    Relative Importance: Conc=0.113, Concur=0.244, Extreme=0.642\n",
      "    → P(Genuine|Data) = 0.750\n",
      "    → ✓ GENUINE: Natural agreement (high W + moderate θ)\n",
      "\n",
      "  Clustered (Outlier)           :\n",
      "    Type: forced, Model: Mallows (forced, dependent)\n",
      "    W=0.718, θ=3.805, MI=1.811, τ=0.549\n",
      "    Relative Importance: Conc=0.113, Concur=0.286, Extreme=0.601\n",
      "    → P(Genuine|Data) = 0.750\n",
      "    → ✓ GENUINE: Natural agreement (high W + moderate θ)\n",
      "\n",
      "================================================================================\n",
      "KEY INSIGHTS\n",
      "================================================================================\n",
      "Traditional Kendall's W CANNOT distinguish:\n",
      "  • Phantom concordance (shared biases) from genuine agreement\n",
      "  • Extreme tail dependence from natural correlation\n",
      "  • Clustered subgroups from uniform agreement\n",
      "\n",
      "CDEF reveals the truth by analyzing:\n",
      "  ✓ Concordance (W): Overall agreement across all raters\n",
      "  ✓ Dependence (θ): Tail dependence & extremeness in rankings\n",
      "  ✓ Concurrence (MI): Shared information structure\n",
      "  ✓ Flexibility (τ): Pairwise correlation patterns\n",
      "\n",
      "Distribution Models:\n",
      "  ✓ Mallows: Forced rankings with dependence (your data!)\n",
      "  ✓ Uniform: Forced rankings with independence\n",
      "  ✓ Multinomial: Non-forced rankings with independence\n",
      "\n",
      "Relative Importance:\n",
      "  • NOT conditional probabilities (different units/scales)\n",
      "  • Normalized decomposition weights showing contribution\n",
      "  • Extremeness dominance indicates tail-dependence drives agreement\n",
      "================================================================================\n",
      "\n",
      "✓ Summary saved to: /mnt/c/Users/lfult/OneDrive - bc.edu/Desktop/Copula/cdef_summary_fixed.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/c/Users/lfult/OneDrive - bc.edu/Desktop/Copula/gumbel_copula_fixed.py:291: UserWarning: Could not fit Gumbel copula for Congrove-NYT: The computed theta value 0.9560508227452614 is out of limits for the given GUMBEL copula.. Using independence (θ=1).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "CDEF Demonstration: Detecting Phantom vs Genuine Concordance\n",
    "\n",
    "Uses the properly fixed Gumbel copula analyzer with:\n",
    "- Auto-detection of forced vs non-forced rankings\n",
    "- Mallows model for forced rankings under dependence\n",
    "- Proper log-likelihoods\n",
    "- Relative importance (not fake conditional probabilities)\n",
    "\"\"\"\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import kendalltau, chi2_contingency, entropy\n",
    "from copulas.bivariate import Gumbel\n",
    "from typing import Dict, Tuple, Optional\n",
    "from dataclasses import dataclass\n",
    "import sys\n",
    "import os\n",
    "\n",
    "# Add parent directory to path to import the fixed analyzer\n",
    "sys.path.insert(0, r'C:\\Users\\lfult\\OneDrive - bc.edu\\Desktop\\Copula')\n",
    "from gumbel_copula_fixed import RankDependencyAnalyzer, CopulaResults\n",
    "\n",
    "\n",
    "def create_phantom_scenario():\n",
    "    \"\"\"\n",
    "    Phantom: HIGH W + VERY HIGH theta + Shared extreme bias\n",
    "    All raters share the SAME extreme bias pattern\n",
    "    \"\"\"\n",
    "    n_teams, n_raters = 136, 4\n",
    "    \n",
    "    # Create identical extreme pattern: alternate top and bottom ranks\n",
    "    extreme_pattern = []\n",
    "    for i in range(n_teams // 2):\n",
    "        extreme_pattern.append(i + 1)  # Top ranks\n",
    "        extreme_pattern.append(n_teams - i)  # Bottom ranks\n",
    "    if n_teams % 2 == 1:\n",
    "        extreme_pattern.append(n_teams // 2 + 1)\n",
    "    \n",
    "    rankings = {}\n",
    "    rater_names = ['CBS', 'CFN', 'Congrove', 'NYT']\n",
    "    \n",
    "    for rater in rater_names:\n",
    "        pattern = extreme_pattern.copy()\n",
    "        # Add tiny variation (2-3 swaps) to avoid perfect correlation\n",
    "        for _ in range(2):\n",
    "            i, j = np.random.choice(n_teams, 2, replace=False)\n",
    "            pattern[i], pattern[j] = pattern[j], pattern[i]\n",
    "        rankings[rater] = pattern\n",
    "    \n",
    "    df = pd.DataFrame(rankings)\n",
    "    df.index.name = 'Team'\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_genuine_scenario():\n",
    "    \"\"\"\n",
    "    Genuine: HIGH W + MODERATE theta + Natural agreement  \n",
    "    Raters give similar rankings without extreme bias\n",
    "    \"\"\"\n",
    "    n_teams = 136\n",
    "    base_pattern = list(range(1, n_teams + 1))\n",
    "    \n",
    "    rankings = {}\n",
    "    rater_names = ['CBS', 'CFN', 'Congrove', 'NYT']\n",
    "    \n",
    "    for rater in rater_names:\n",
    "        pattern = base_pattern.copy()\n",
    "        # Add moderate noise (10-15 swaps)\n",
    "        for _ in range(12):\n",
    "            i, j = np.random.choice(n_teams, 2, replace=False)\n",
    "            pattern[i], pattern[j] = pattern[j], pattern[i]\n",
    "        rankings[rater] = pattern\n",
    "    \n",
    "    df = pd.DataFrame(rankings)\n",
    "    df.index.name = 'Team'\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_random_scenario():\n",
    "    \"\"\"Random: LOW W + LOW theta + No agreement\"\"\"\n",
    "    n_teams = 136\n",
    "    \n",
    "    rankings = {}\n",
    "    rater_names = ['CBS', 'CFN', 'Congrove', 'NYT']\n",
    "    \n",
    "    for rater in rater_names:\n",
    "        rankings[rater] = list(np.random.permutation(range(1, n_teams + 1)))\n",
    "    \n",
    "    df = pd.DataFrame(rankings)\n",
    "    df.index.name = 'Team'\n",
    "    return df\n",
    "\n",
    "\n",
    "def create_clustered_scenario():\n",
    "    \"\"\"\n",
    "    Clustered: HIGH W (among 3) + One divergent rater\n",
    "    Mimics the Congrove pattern from real data\n",
    "    \"\"\"\n",
    "    n_teams = 136\n",
    "    base_pattern = list(range(1, n_teams + 1))\n",
    "    \n",
    "    rankings = {}\n",
    "    \n",
    "    # CBS, CFN, NYT - tight cluster (small variations)\n",
    "    for rater in ['CBS', 'CFN', 'NYT']:\n",
    "        pattern = base_pattern.copy()\n",
    "        for _ in range(8):\n",
    "            i, j = np.random.choice(n_teams, 2, replace=False)\n",
    "            pattern[i], pattern[j] = pattern[j], pattern[i]\n",
    "        rankings[rater] = pattern\n",
    "    \n",
    "    # Congrove - independent (large variations)\n",
    "    pattern = base_pattern.copy()\n",
    "    for _ in range(40):\n",
    "        i, j = np.random.choice(n_teams, 2, replace=False)\n",
    "        pattern[i], pattern[j] = pattern[j], pattern[i]\n",
    "    rankings['Congrove'] = pattern\n",
    "    \n",
    "    df = pd.DataFrame(rankings)\n",
    "    df.index.name = 'Team'\n",
    "    return df\n",
    "\n",
    "\n",
    "def cdef_interpretation(results: CopulaResults) -> Tuple[float, str]:\n",
    "    \"\"\"\n",
    "    Interpret CDEF results to classify as Phantom, Genuine, Random, or Clustered.\n",
    "    \n",
    "    Uses:\n",
    "    - W (concordance): Overall agreement\n",
    "    - theta (extremeness): Tail dependence\n",
    "    - MI (concurrence): Shared information\n",
    "    - avg_tau: Pairwise correlations\n",
    "    \n",
    "    Returns:\n",
    "        Tuple of (probability_genuine, interpretation_string)\n",
    "    \"\"\"\n",
    "    W = results.kendalls_W\n",
    "    theta = results.theta_scaled\n",
    "    mi = results.mutual_information\n",
    "    avg_tau = results.avg_kendalls_tau\n",
    "    \n",
    "    # Phantom detection: Very high W + Very high theta = shared extreme bias\n",
    "    if W > 0.85 and theta > 15.0:\n",
    "        prob = 0.10\n",
    "        interpretation = \"⚠️ PHANTOM: Shared extreme biases (very high W + very high θ)\"\n",
    "    \n",
    "    # Strong phantom\n",
    "    elif W > 0.75 and theta > 8.0:\n",
    "        prob = 0.20\n",
    "        interpretation = \"⚠️ Likely PHANTOM: High concordance with extreme tail dependence\"\n",
    "    \n",
    "    # Genuine agreement: High W + Moderate theta\n",
    "    elif W > 0.65 and 2.5 < theta < 6.5:\n",
    "        prob = 0.75\n",
    "        interpretation = \"✓ GENUINE: Natural agreement (high W + moderate θ)\"\n",
    "    \n",
    "    # Strong genuine: Very high W + Low-moderate theta\n",
    "    elif W > 0.75 and theta < 4.0:\n",
    "        prob = 0.85\n",
    "        interpretation = \"✓✓ STRONG GENUINE: Excellent natural agreement\"\n",
    "    \n",
    "    # Clustered (some agree, some don't)\n",
    "    elif W > 0.5 and 0.3 < avg_tau < 0.65:\n",
    "        prob = 0.60\n",
    "        interpretation = \"→ CLUSTERED: Subgroup agreement with divergent rater(s)\"\n",
    "    \n",
    "    # Weak agreement\n",
    "    elif W > 0.25:\n",
    "        prob = 0.70\n",
    "        interpretation = \"◐ WEAK: Limited systematic agreement\"\n",
    "    \n",
    "    # Random/independence\n",
    "    else:\n",
    "        prob = 0.90\n",
    "        interpretation = \"○ RANDOM: No systematic agreement (independence)\"\n",
    "    \n",
    "    return prob, interpretation\n",
    "\n",
    "\n",
    "def save_scenario_to_excel(rankings_df: pd.DataFrame, filename: str, scenario_name: str):\n",
    "    \"\"\"Save scenario in long format matching original data structure\"\"\"\n",
    "    long_format = []\n",
    "    \n",
    "    for team_idx in range(len(rankings_df)):\n",
    "        for rater in rankings_df.columns:\n",
    "            long_format.append({\n",
    "                'Ratee': f'Team_{team_idx+1}',\n",
    "                'Rater': rater,\n",
    "                'Ranking': int(rankings_df.iloc[team_idx][rater])\n",
    "            })\n",
    "    \n",
    "    df_long = pd.DataFrame(long_format)\n",
    "    output_path = f\"/mnt/c/Users/lfult/OneDrive - bc.edu/Desktop/Copula/{filename}\"\n",
    "\n",
    "    df_long.to_excel(output_path, sheet_name='Sheet1', index=False)\n",
    "    print(f\"  Saved to: {filename}\")\n",
    "\n",
    "\n",
    "def run_cdef_demonstration():\n",
    "    \"\"\"\n",
    "    Demonstrate CDEF's diagnostic capability using proper Gumbel copula analysis.\n",
    "    \n",
    "    Shows how CDEF distinguishes:\n",
    "    - Phantom concordance (shared biases) from genuine agreement\n",
    "    - Extreme tail dependence from natural correlation\n",
    "    - Clustered subgroups from uniform agreement\n",
    "    \"\"\"\n",
    "    \n",
    "    np.random.seed(42)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"CDEF DEMONSTRATION: Detecting Phantom vs Genuine Concordance\")\n",
    "    print(\"Using Properly Fixed Gumbel Copula Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    scenarios = {\n",
    "        'Phantom (Extreme Bias)': create_phantom_scenario(),\n",
    "        'Genuine (Natural Agreement)': create_genuine_scenario(),\n",
    "        'Random (No Agreement)': create_random_scenario(),\n",
    "        'Clustered (Outlier)': create_clustered_scenario()\n",
    "    }\n",
    "    \n",
    "    results_list = []\n",
    "    \n",
    "    for name, rankings_df in scenarios.items():\n",
    "        print(f\"\\n{'=' * 80}\")\n",
    "        print(f\"{name.upper()}\")\n",
    "        print('=' * 80)\n",
    "        \n",
    "        # Save scenario to Excel\n",
    "        filename = f\"scenario_{name.lower().replace(' ', '_').replace('(', '').replace(')', '')}.xlsx\"\n",
    "        save_scenario_to_excel(rankings_df, filename, name)\n",
    "        \n",
    "        # Create temporary Excel file for analysis\n",
    "        temp_file = f'/tmp/temp_{filename}'\n",
    "        long_format = []\n",
    "        for team_idx in range(len(rankings_df)):\n",
    "            for rater in rankings_df.columns:\n",
    "                long_format.append({\n",
    "                    'Ratee': f'Team_{team_idx+1}',\n",
    "                    'Rater': rater,\n",
    "                    'Ranking': int(rankings_df.iloc[team_idx][rater])\n",
    "                })\n",
    "        df_long = pd.DataFrame(long_format)\n",
    "        df_long.to_excel(temp_file, sheet_name='Sheet1', index=False)\n",
    "        \n",
    "        # Run analysis with proper analyzer\n",
    "        analyzer = RankDependencyAnalyzer(random_seed=42)\n",
    "        try:\n",
    "            results = analyzer.analyze_from_excel(\n",
    "                temp_file, 'Sheet1', 'Rater', 'Ratee', 'Ranking'\n",
    "            )\n",
    "        except Exception as e:\n",
    "            print(f\"Error analyzing {name}: {e}\")\n",
    "            continue\n",
    "        \n",
    "        # CDEF interpretation\n",
    "        cdef_prob, cdef_interp = cdef_interpretation(results)\n",
    "        \n",
    "        # Traditional interpretation (W only)\n",
    "        if results.kendalls_W > 0.7:\n",
    "            traditional = \"High concordance → Good agreement\"\n",
    "        elif results.kendalls_W > 0.4:\n",
    "            traditional = \"Moderate concordance → Some agreement\"\n",
    "        else:\n",
    "            traditional = \"Low concordance → Poor agreement\"\n",
    "        \n",
    "        # Display results\n",
    "        print(f\"\\nRanking Type: {results.ranking_type}\")\n",
    "        print(f\"Distribution Model: {results.distribution_model}\")\n",
    "        if results.model_log_likelihood is not None:\n",
    "            print(f\"Model Log-Likelihood: {results.model_log_likelihood}\")\n",
    "        \n",
    "        print(f\"\\nCore Metrics:\")\n",
    "        print(f\"  Kendall's W (concordance):     {results.kendalls_W:.3f}\")\n",
    "        print(f\"  Theta (scaled):                {results.theta_scaled:.3f}\")\n",
    "        print(f\"  Gumbel theta (from tau):       {results.theta_gumbel:.3f}\")\n",
    "        print(f\"  Avg Kendall's tau:             {results.avg_kendalls_tau:.3f}\")\n",
    "        print(f\"  Mutual information:            {results.mutual_information:.3f}\")\n",
    "        \n",
    "        print(f\"\\nLog-Likelihoods:\")\n",
    "        print(f\"  Copula (average):              {results.avg_log_likelihood:.6f}\")\n",
    "        print(f\"  Independence baseline:         {results.independence_log_likelihood:.6f}\")\n",
    "        \n",
    "        print(f\"\\nRelative Importance (NOT probabilities):\")\n",
    "        for key, val in results.relative_importance.items():\n",
    "            print(f\"  {key:15s}: {val:.3f}\")\n",
    "        \n",
    "        print(f\"\\nPairwise Theta Range:\")\n",
    "        theta_values = list(results.pairwise_thetas.values())\n",
    "        print(f\"  Max: {max(theta_values):.3f}, Min: {min(theta_values):.3f}\")\n",
    "        \n",
    "        print(f\"\\nTraditional Analysis (W only):\")\n",
    "        print(f\"  {traditional}\")\n",
    "        \n",
    "        print(f\"\\nCDEF Analysis (W + θ + MI + τ):\")\n",
    "        print(f\"  P(Genuine|Data) = {cdef_prob:.3f}\")\n",
    "        print(f\"  Interpretation: {cdef_interp}\")\n",
    "        \n",
    "        # Store results\n",
    "        result = {\n",
    "            'Scenario': name,\n",
    "            'Ranking_Type': results.ranking_type,\n",
    "            'Model': results.distribution_model,\n",
    "            'W': results.kendalls_W,\n",
    "            'Theta_scaled': results.theta_scaled,\n",
    "            'Theta_gumbel': results.theta_gumbel,\n",
    "            'Avg_tau': results.avg_kendalls_tau,\n",
    "            'MI': results.mutual_information,\n",
    "            'Copula_LL': results.avg_log_likelihood,\n",
    "            'Model_LL': results.model_log_likelihood,\n",
    "            'Rel_Concordance': results.relative_importance['Concordance'],\n",
    "            'Rel_Concurrence': results.relative_importance['Concurrence'],\n",
    "            'Rel_Extremeness': results.relative_importance['Extremeness'],\n",
    "            'Traditional': traditional,\n",
    "            'P_Genuine': cdef_prob,\n",
    "            'CDEF_Interpretation': cdef_interp\n",
    "        }\n",
    "        results_list.append(result)\n",
    "        \n",
    "        # Clean up temp file\n",
    "        if os.path.exists(temp_file):\n",
    "            os.remove(temp_file)\n",
    "    \n",
    "    # Summary comparison table\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"SUMMARY: Traditional vs CDEF Analysis\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    df_results = pd.DataFrame(results_list)\n",
    "    \n",
    "    print(\"\\nTraditional Analysis (Kendall's W only):\")\n",
    "    for _, row in df_results.iterrows():\n",
    "        print(f\"  {row['Scenario']:30s}: W={row['W']:.3f} → {row['Traditional']}\")\n",
    "    \n",
    "    print(\"\\nCDEF Copula Analysis (Full Dependency Structure):\")\n",
    "    for _, row in df_results.iterrows():\n",
    "        print(f\"\\n  {row['Scenario']:30s}:\")\n",
    "        print(f\"    Type: {row['Ranking_Type']}, Model: {row['Model']}\")\n",
    "        print(f\"    W={row['W']:.3f}, θ={row['Theta_scaled']:.3f}, MI={row['MI']:.3f}, τ={row['Avg_tau']:.3f}\")\n",
    "        print(f\"    Relative Importance: Conc={row['Rel_Concordance']:.3f}, \"\n",
    "              f\"Concur={row['Rel_Concurrence']:.3f}, Extreme={row['Rel_Extremeness']:.3f}\")\n",
    "        print(f\"    → P(Genuine|Data) = {row['P_Genuine']:.3f}\")\n",
    "        print(f\"    → {row['CDEF_Interpretation']}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"KEY INSIGHTS\")\n",
    "    print(\"=\" * 80)\n",
    "    print(\"Traditional Kendall's W CANNOT distinguish:\")\n",
    "    print(\"  • Phantom concordance (shared biases) from genuine agreement\")\n",
    "    print(\"  • Extreme tail dependence from natural correlation\")\n",
    "    print(\"  • Clustered subgroups from uniform agreement\")\n",
    "    print(\"\\nCDEF reveals the truth by analyzing:\")\n",
    "    print(\"  ✓ Concordance (W): Overall agreement across all raters\")\n",
    "    print(\"  ✓ Dependence (θ): Tail dependence & extremeness in rankings\")\n",
    "    print(\"  ✓ Concurrence (MI): Shared information structure\")\n",
    "    print(\"  ✓ Flexibility (τ): Pairwise correlation patterns\")\n",
    "    print(\"\\nDistribution Models:\")\n",
    "    print(\"  ✓ Mallows: Forced rankings with dependence (your data!)\")\n",
    "    print(\"  ✓ Uniform: Forced rankings with independence\")\n",
    "    print(\"  ✓ Multinomial: Non-forced rankings with independence\")\n",
    "    print(\"\\nRelative Importance:\")\n",
    "    print(\"  • NOT conditional probabilities (different units/scales)\")\n",
    "    print(\"  • Normalized decomposition weights showing contribution\")\n",
    "    print(\"  • Extremeness dominance indicates tail-dependence drives agreement\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return df_results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    results_df = run_cdef_demonstration()\n",
    "    \n",
    "    # Save summary\n",
    "    output_file = '/mnt/c/Users/lfult/OneDrive - bc.edu/Desktop/Copula/cdef_summary_fixed.csv'\n",
    "    results_df.to_csv(output_file, index=False)\n",
    "    print(f\"\\n✓ Summary saved to: {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccc8423c-1c69-4eb4-829e-3856837e5614",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (torch-env)",
   "language": "python",
   "name": "torch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
